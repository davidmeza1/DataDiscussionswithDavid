{
	"nodes":[
		{"type":"text","text":"Private LLM Structure","id":"169a9dfb5d945d46","x":-380,"y":-300,"width":250,"height":60},
		{"type":"text","text":"Open Source LLM\nThese are small [open-source alternatives to ChatGPT](https://bdtechtalks.com/2023/04/17/open-source-chatgpt-alternatives/) that can be run on your local machine. Some popular examples include Dolly, Vicuna, [GPT4All](https://gpt4all.io/index.html), and [llama.cpp](https://github.com/ggerganov/llama.cpp). These models are trained on large amounts of text and can generate high-quality responses to user prompts.","id":"ab180097c14cf87a","x":-435,"y":-140,"width":360,"height":205},
		{"type":"text","text":"Embedding model\nAn embedding model is used to transform text data into a numerical format that can be easily compared to other text data. This is typically done using a technique called word or sentence embeddings, which represent text as dense vectors in a high-dimensional space. These embeddings can be used to find documents that are related to the user’s prompt. The [SentenceTransformers](https://www.sbert.net/index.html) library contains a rich variety of pre-trained embedding models.","id":"1f95900d89088ec4","x":-435,"y":140,"width":360,"height":220},
		{"type":"text","text":"Vector Database\nA vector database is designed to store and retrieve embeddings. It can store the content of your documents in a format that can be easily compared to the user’s prompt. [Faiss](https://github.com/facebookresearch/faiss) is a library that you can use to add vector similarity comparisons on top of other data stores. But there are also a few open-source vector databases that you can install on your computer including [Qdrant](https://qdrant.tech/), Weaviate, and [Milvus](https://github.com/milvus-io/milvus).","id":"fb7088763c01b4b1","x":-435,"y":460,"width":360,"height":186},
		{"type":"text","text":"Knowledge Documents\nA collection of documents that contain the knowledge your LLM will use to answer your questions. These documents depend on your application. For example, it can be a collection of PDF or text documents that contain your personal blog posts.","id":"6ee652bb6b623b5e","x":-435,"y":724,"width":360,"height":196},
		{"type":"text","text":"User Interface\nThe user interface layer will take user prompts and display the model’s output. This can be a simple command-line interface (CLI) or a more sophisticated web application such as [Streamlit](https://github.com/streamlit/streamlit). The user interface will send the user’s prompt to the application and return he model’s response to the user.","id":"6c6a13d219e493e6","x":-435,"y":1004,"width":360,"height":216},
		{"type":"file","file":"private-LLM-architecture.webp","id":"6afb6f76709ecaee","x":-435,"y":1360,"width":360,"height":225},
		{"type":"text","text":"Private LLM Workflow","id":"4f5075066b35cdcd","x":100,"y":-300,"width":250,"height":60},
		{"type":"text","text":"Before you can use your local LLM, you must make a few preparations:\n\n1. Create a list of documents that you want to use as your knowledge base\n\n2. Break large documents into smaller chunks (around 500 words)\n\n3. Create an embedding for each document chunk\n\n4. Create a vector database that stores all the embeddings of the documents","id":"433d78959fe942de","x":65,"y":-140,"width":320,"height":205},
		{"type":"text","text":"Now that our knowledge base and vector database are ready, we can review the workflow of the private LLM:\n\n1. The user enters a prompt in the user interface.\n\n2. The application uses the embedding model to create an embedding from the user’s prompt and send it to the vector database.\n\n3. The vector database returns a list of documents that are relevant to the prompt based on the similarity of their embeddings to the user’s prompt.\n\n4. The application creates a new prompt with the user’s initial prompt and the retrieved documents as context and sends it to the local LLM.\n\n5. The LLM produces the result along with citations from the context documents. The result is displayed in the user interface along with the sources.","id":"c3c604ef94ef5e5d","x":65,"y":140,"width":320,"height":220},
		{"type":"file","file":"private-LLM-ChatGPT-workflow.webp","id":"13e390b486cc52bd","x":25,"y":474,"width":400,"height":250},
		{"type":"text","text":"One solution is [PrivateGPT](https://github.com/imartinez/privateGPT), a project hosted on GitHub that brings together all the components mentioned above in an easy-to-install package. PrivateGPT includes a language model, an embedding model, a database for document embeddings, and a command-line interface. It supports several types of documents including plain text (.txt), comma-separated values (.csv), Word (.docx and .doc), PDF, Markdown (.md), HTML, Epub, and email files (.eml and .msg).","id":"893b138d16c7b694","x":540,"y":-140,"width":340,"height":205},
		{"type":"text","text":"Private GPT Solution on Github","id":"6c37c7df7baf9a0a","x":585,"y":-300,"width":250,"height":60},
		{"type":"text","text":"To use PrivateGPT, you’ll need Python installed on your computer. You can start by cloning the PrivateGPT repository on your computer and install the requirements:\n\n```\ngit clone https://github.com/imartinez/privateGPT.git\ncd privateGPT/\npip install -r requirements.txt\n```","id":"d7f34239fe9e1a35","x":540,"y":140,"width":340,"height":220},
		{"type":"text","text":"Next, you need to download a pre-trained language model on your computer. Create a “models” folder in the PrivateGPT directory and move the model file to this folder. PrivateGPT is configured by default to work with GPT4ALL-J (you can download it [here](https://gpt4all.io/models/ggml-gpt4all-j-v1.3-groovy.bin)) but it also supports llama.cpp. These are both open-source LLMs that have been trained for instruction-following (like ChatGPT). They have also been designed to run on computers with consumer-grade hardware. Llama.cpp works especially well on Mac computers with M1 processors.","id":"2042545f6ba80e4d","x":540,"y":474,"width":340,"height":250},
		{"type":"text","text":"Next, you have to create your knowledge base. PrivateGPT has a “source_documents” folder where you must copy all your documents. After that, you must populate your vector database with the embedding values of your documents. Fortunately, the project has a script that performs the entire process of breaking documents into chunks, creating embeddings, and storing them in the vector database:\n\n```\npython ingest.py\n```\n\nBehind the scenes, PrivateGPT uses LangChain and SentenceTransformers to break the documents into 500-token chunks and generate embeddings. And it uses DuckDB to create the vector database. The result is stored in the project’s “db” folder. One thing to note is that LangChain needs to be connected to the internet to download the pre-trained embedding model. After that, all processing takes place on your own computer and you don’t need internet connectivity.","id":"01d981df1a21ca5a","x":540,"y":805,"width":340,"height":255},
		{"type":"text","text":"Depending on the number of documents that you have, creating the vector database might take several minutes. Once the preparation is finished, you can start the model with the following command:\n\n```\npython privateGPT.py\n```\n\nAnd then you can start talking to your local LLM with no strings attached. It will answer your questions and provide up to four sources from your knowledge base for each reply. PrivateGPT is an experimental project. It is not fast (it can take 20-30 seconds to respond) and is not optimized for every type of hardware. Its installation might also run into bugs based on your operating system and hardware. But it is surely a preview of one of the many directions the field is taking and the [powerful applications that open-source LLMs can unlock](https://bdtechtalks.com/2023/05/29/open-source-llms-cerebras-gpt/).","id":"725a2994bc0dd3cf","x":540,"y":1125,"width":340,"height":255},
		{"type":"text","text":"[# How to create a private ChatGPT that interacts with your local documents](https://bdtechtalks.com/2023/06/01/create-privategpt-local-llm/)","id":"548414f7611ddc10","x":-75,"y":-480,"width":835,"height":93},
		{"type":"text","text":"[# How to create a private ChatGPT with your own data](https://medium.com/@imicknl/how-to-create-a-private-chatgpt-with-your-own-data-15754e6378a1)","id":"90650ae7313711dd","x":1560,"y":-480,"width":566,"height":78,"color":"6"},
		{"type":"text","text":"# 1. Disadvantages of finetuning a LLM with your own data\n\nOften people refer to finetuning (training) as a solution for adding your own data on top of a pretrained language model. However, this has drawbacks like risk of hallucinations as mentioned during the [recent GPT-4 announcement](https://openai.com/research/gpt-4). Next to that, GPT-4 has only been trained with data up to September 2021.\n\nCommon drawbacks when you finetune a LLM;\n\n- Factual correctness and traceability, _where does the answer come from_\n- Access control, _impossible to limit certain documents to specific users or groups_\n- Costs, _new documents require retraining of the model and model hosting_\n\nThis will make it extremely hard, close to impossible, to use fine-tuning for the purpose of Question Answering (QA). How can we overcome such limitations and still benefit from these LLMs?","id":"840698a8c7d9c3c4","x":1560,"y":-231,"width":566,"height":431},
		{"type":"text","text":"# 2. Separate your knowledge from your language model\n\nTo ensure that users receive accurate answers, we need to separate our language model from our knowledge base. This allows us to leverage the semantic understanding of our language model while also providing our users with the most relevant information. All of this happens in real-time, and no model training is required.\n\nIt might seem like a good idea to feed all documents to the model during run-time, but this isn’t feasible due to the character limit ([measured in tokens](https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them)) that can be processed at once. For example, GPT-3 supports up to 4K tokens, GPT-4 up to 8K or 32K tokens. Since pricing is per 1000 tokens, using fewer tokens can help to save costs as well.\n\nThe approach for this would be as follows:\n\n1. User asks a question\n2. Application finds the most relevant text that (most likely) contains the answer\n3. A concise prompt with relevant document text is sent to the LLM\n4. User will receive an answer or ‘No answer found’ response","id":"1696d10798a21eff","x":1560,"y":347,"width":566,"height":377},
		{"type":"file","file":"RetrievalAugmentationGeneration.webp","id":"eef6c1fa708c3606","x":1483,"y":840,"width":720,"height":340},
		{"type":"text","text":"This approach is often referred to as grounding the model or [Retrieval Augmented Generation](https://arxiv.org/abs/2005.11401) (RAG). The application will provide additional context to the language model, to be able to answer the question based on relevant resources.","id":"fcd21f2bc285bc55","x":1560,"y":1220,"width":566,"height":114},
		{"type":"text","text":"# 3. Retrieve the most relevant data\n\nContext is key. To ensure the language model has the right information to work with, we need to build a knowledge base that can be used to find the most relevant documents through semantic search. This will enable us to provide the language model with the right context, allowing it to generate the right answer.\n\n## 3.1 Chunk and split your data\n\nSince the answering prompt has a token limit, we need to make sure we cut our documents in smaller chunks. Depending on the size of your chunk, you could also share multiple relevant sections and generate an answer over multiple documents.\n\nWe can start by simply splitting the document per page, or by using [a text splitter](https://langchain.readthedocs.io/en/latest/reference/modules/text_splitter.html) that splits on a set token length. When we have our documents in a more accessible format, it is time to create a search index that can be queried by providing it with a user question.\n\nNext to these chunks, you should add additional metadata to your index. Store the original source and page number to link the answer to your original document. Store additional metadata that can be used for access control and filtering.\n## option 1: use a search product\n\nThe easiest way to build a semantic search index is to leverage an existing Search as a Service platform. On Azure, you can for example use Cognitive Search which offers a managed document ingestion pipeline and [semantic ranking](https://learn.microsoft.com/en-us/azure/search/semantic-ranking) leveraging the language models behind Bing.\n\n## option 2: use embeddings to build your own semantic search\n\n> An embedding is a vector (list) of floating point numbers. The [distance](https://platform.openai.com/docs/guides/embeddings/which-distance-function-should-i-use) between two vectors measures their relatedness. Small distances suggest high relatedness and large distances suggest low relatedness. [1]\n\nIf you want to leverage the latest semantic models and have more control over your search index, you could use the text embedding models from OpenAI. For all your sections you will need to precompute embeddings and store them.\n\nOn Azure you can store these embeddings in a managed vector database like [Cognitive Search with Vector Search (preview)](https://github.com/Azure/cognitive-search-vector-pr), Azure Cache for Redis (RediSearch) or in a open source vector database like Weaviate or Pinecone. During the application run-time you will first turn the user question into an embedding, so we can compare the cosine similarity of the question embedding with the document embeddings we generated earlier. Advanced search products like Cognitive Search can do a hybrid search where the best of both keyword search and vector search is combined.\n## 3.2 Improve relevancy with different chunking strategies\n\nTo be able to find the most relevant information, it is important that you understand your data and potential user queries. What kind of data do you need to answer the question? This will decide how your data can be best split.\n\nCommon patterns that might improve relevancy are:\n\n- **Use a sliding window**; chunking per page or per token can have the unwanted effect of losing context. Use a sliding window to have overlapping content in your chunks, to increase the chance of having the most relevant information in a chunk.\n- **Provide more context**; a very structured document with sections that nest multiple levels deep (e.g. section 1.3.3.7) could benefit from extra context like the chapter and section title. You could parse these sections and add this context to every chunk.\n- **Summarization**, create chunks that contain a summary of a larger document section. This will allow us to capture the most essential text and bring this all together in one chunk.","id":"dc3240b2d58d2d66","x":1560,"y":1411,"width":566,"height":349},
		{"type":"file","file":"LLM_embeddings.webp","id":"9748bdf506372b05","x":2260,"y":1446,"width":672,"height":280},
		{"type":"text","text":"# 4. Write a concise prompt to avoid hallucination\n\n> [Designing your prompt](https://platform.openai.com/docs/guides/completion/prompt-design) is how you “program” the model, usually by providing some instructions or a few examples. [2]\n\nYour prompt is an essential part of your ChatGPT implementation to prevent unwanted responses. Nowadays, people call prompt engineering a new skill and more and more samples are shared every week.\n\nIn your prompt you want to be clear that the model should be concise and only use data from the provided context. When it cannot answer the question, it should provide a predefined ‘no answer’ response. The output should include a footnote (citations) to the original document, to allow the user to verify its factual accuracy by looking at the source.\n\nOne-shot learning is used to enhance the response; we provide an example of how a user question should be handled and we provide sources with a unique identifier and an example answer that is composed by text from multiple sources. During runtime `{q}` will be populated by the user question and `{retrieved}` will be populated by the relevant sections from your knowledge base, for your final prompt.\n\nDon’t forget to set a low temperature via your parameters if you want a more repetitive and deterministic response. Increasing the temperature will result in more unexpected or creative responses.\n\nThis prompt is eventually used to generate a response via the (Azure) OpenAI API. If you use the gpt-35-turbo model (ChatGPT) you can pass the conversation history in every turn to be able to ask clarifying questions or use other reasoning tasks (e.g. summarization). A great resource to learn more about prompt engineering is [dair-ai/Prompt-Engineering-Guide](https://github.com/dair-ai/Prompt-Engineering-Guide) on GitHub.","id":"f970ce2c5cd37ff3","x":1560,"y":1826,"width":566,"height":294},
		{"type":"text","text":"# 5. Next steps\n\nIn this article, I did discuss the architecture and design patterns needed to build an implementation, without delving into the specifics of the code. These patterns are commonly used nowadays, and the following projects and notebooks can serve as inspiration to help you start building such a solution.\n\n- [**Azure OpenAI Service — On Your Data**](https://techcommunity.microsoft.com/t5/ai-cognitive-services-blog/introducing-azure-openai-service-on-your-data-in-public-preview/ba-p/3847000), new feature that allows you to combine OpenAI models, such as ChatGPT and GPT-4, with your own data in a fully managed way. No complex infrastructure or code required.\n- [**ChatGPT Retrieval Plugin**](https://github.com/openai/chatgpt-retrieval-plugin), let ChatGPT access up-to-date information. For now, this only supports the public ChatGPT, but hopefully the capability to add plugins will be added to the ChatGPT API (OpenAI + Azure) in the future.\n- [**LangChain**](https://github.com/hwchase17/langchain)**,** popular library to combine LLMs and other sources of computation or knowledge\n- [**Azure Cognitive Search + OpenAI accelerator**](https://github.com/Azure-Samples/azure-search-openai-demo), ChatGPT-like experience over your own data, ready to deploy\n- [**OpenAI Cookbook**](https://github.com/openai/openai-cookbook/blob/main/examples/Question_answering_using_embeddings.ipynb)**,** example of how to leverage OpenAI embeddings for Q&A in a Jupyter notebook (no infrastructure required)\n- [**Semantic Kernel**](https://devblogs.microsoft.com/semantic-kernel/hello-world/), new library to mix conventional programming languages with LLMs (prompt templating, chaining, and planning capabilities)\n\nEventually, you can look into extending ‘your own ChatGPT’ by linking it to more systems and capabilities via tools like LangChain or Semantic Kernel. The possibilities are endless.","id":"a7a645bd39ec6540","x":1560,"y":2180,"width":566,"height":340},
		{"type":"file","file":"LLM_embeddings2.webp","id":"e54f041737f2e6e4","x":2260,"y":2193,"width":672,"height":315}
	],
	"edges":[
		{"id":"ef28da4ca7fc84fa","fromNode":"169a9dfb5d945d46","fromSide":"bottom","toNode":"ab180097c14cf87a","toSide":"top"},
		{"id":"65aecf834de583b2","fromNode":"ab180097c14cf87a","fromSide":"bottom","toNode":"1f95900d89088ec4","toSide":"top"},
		{"id":"d4fee8dceb0960ee","fromNode":"1f95900d89088ec4","fromSide":"bottom","toNode":"fb7088763c01b4b1","toSide":"top"},
		{"id":"60a01d26700748d5","fromNode":"fb7088763c01b4b1","fromSide":"bottom","toNode":"6ee652bb6b623b5e","toSide":"top"},
		{"id":"69d82c9d5db27716","fromNode":"6ee652bb6b623b5e","fromSide":"bottom","toNode":"6c6a13d219e493e6","toSide":"top"},
		{"id":"1095b10615bf1abe","fromNode":"6c6a13d219e493e6","fromSide":"bottom","toNode":"6afb6f76709ecaee","toSide":"top"},
		{"id":"612cd940de840d84","fromNode":"4f5075066b35cdcd","fromSide":"bottom","toNode":"433d78959fe942de","toSide":"top"},
		{"id":"f284fc8309b65ef6","fromNode":"433d78959fe942de","fromSide":"bottom","toNode":"c3c604ef94ef5e5d","toSide":"top"},
		{"id":"ac274f7dc06bf332","fromNode":"c3c604ef94ef5e5d","fromSide":"bottom","toNode":"13e390b486cc52bd","toSide":"top"},
		{"id":"21dc000d5749a395","fromNode":"6c37c7df7baf9a0a","fromSide":"bottom","toNode":"893b138d16c7b694","toSide":"top"},
		{"id":"59db575d4c8e73ef","fromNode":"893b138d16c7b694","fromSide":"bottom","toNode":"d7f34239fe9e1a35","toSide":"top"},
		{"id":"54c4cef7e8efa6bc","fromNode":"d7f34239fe9e1a35","fromSide":"bottom","toNode":"2042545f6ba80e4d","toSide":"top"},
		{"id":"8b8f9fb5802c7beb","fromNode":"2042545f6ba80e4d","fromSide":"bottom","toNode":"01d981df1a21ca5a","toSide":"top"},
		{"id":"a12e795d491dd974","fromNode":"01d981df1a21ca5a","fromSide":"bottom","toNode":"725a2994bc0dd3cf","toSide":"top"},
		{"id":"5aca7756cd0fd8bc","fromNode":"6c6a13d219e493e6","fromSide":"right","toNode":"4f5075066b35cdcd","toSide":"left"},
		{"id":"1b2ef81779ca783a","fromNode":"c3c604ef94ef5e5d","fromSide":"right","toNode":"6c37c7df7baf9a0a","toSide":"left"},
		{"id":"84b465aaab97b80e","fromNode":"548414f7611ddc10","fromSide":"bottom","toNode":"169a9dfb5d945d46","toSide":"top"},
		{"id":"5295b7bbb8fd5354","fromNode":"90650ae7313711dd","fromSide":"bottom","toNode":"840698a8c7d9c3c4","toSide":"top"},
		{"id":"8db3d1b942e7acb1","fromNode":"840698a8c7d9c3c4","fromSide":"bottom","toNode":"1696d10798a21eff","toSide":"top"},
		{"id":"63c5e65ae9071725","fromNode":"1696d10798a21eff","fromSide":"bottom","toNode":"eef6c1fa708c3606","toSide":"top"},
		{"id":"43547afa9497337b","fromNode":"eef6c1fa708c3606","fromSide":"bottom","toNode":"fcd21f2bc285bc55","toSide":"top"},
		{"id":"bba87113ab3455da","fromNode":"fcd21f2bc285bc55","fromSide":"bottom","toNode":"dc3240b2d58d2d66","toSide":"top"},
		{"id":"587308de9656bb6e","fromNode":"dc3240b2d58d2d66","fromSide":"right","toNode":"9748bdf506372b05","toSide":"left"},
		{"id":"1ceebcecd53baf9f","fromNode":"dc3240b2d58d2d66","fromSide":"bottom","toNode":"f970ce2c5cd37ff3","toSide":"top"},
		{"id":"cf57691b9e41271c","fromNode":"f970ce2c5cd37ff3","fromSide":"bottom","toNode":"a7a645bd39ec6540","toSide":"top"},
		{"id":"8dd850bca829cd4d","fromNode":"a7a645bd39ec6540","fromSide":"right","toNode":"e54f041737f2e6e4","toSide":"left"}
	]
}