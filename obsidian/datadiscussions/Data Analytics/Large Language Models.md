[Understanding Large Language Models](https://magazine.sebastianraschka.com/p/understanding-large-language-models)

[Llama](https://huggingface.co/docs/transformers/main/model_doc/llama)
The LLaMA model was proposed in [LLaMA: Open and Efficient Foundation Language Models](LLaMA: Open and Efficient Foundation Language Models) by Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample. It is a collection of foundation language models ranging from 7B to 65B parameters.

[Alpaca](https://crfm.stanford.edu/2023/03/13/alpaca.html)
[alapca_gihub](https://github.com/tatsu-lab/stanford_alpaca#fine-tuning)



[Dalai](https://github.com/cocktailpeanut/dalai)

I have installed this on laptop, it is located in my dmeza folder

[dockerized Dalai](https://github.com/MKAbuMattar/dockerized-dalai)
This repository contains the Dockerfile and the scripts to build the Docker image for the [DalAI](https://github.com/cocktailpeanut/dalai) project, documented in the [DalAI documentation](https://cocktailpeanut.github.io/dalai/).

As of April 17, 2023 I have not done this


[Alpaca Electron](https://github.com/ItsPi3141/alpaca-electron)
Installed on laptop, the app comes up, but it runs very slowly.
I have downloaded several models to my usb drive

[Models from Hugging Face](https://huggingface.co/Pi3141)

[GPT4ALL](https://github.com/nomic-ai/gpt4all)
I have downloaded to usb drive, but have not tried it yet

[Dolly](https://huggingface.co/databricks/dolly-v1-6b)

[Auto GPT](https://github.com/Significant-Gravitas/Auto-GPT)
Auto-GPT is an experimental open-source application showcasing the capabilities of the GPT-4 language model. This program, driven by GPT-4, chains together LLM "thoughts", to autonomously achieve whatever goal you set. As one of the first examples of GPT-4 running fully autonomously, Auto-GPT pushes the boundaries of what is possible with AI.


[MetaGPT](https://github.com/geekan/MetaGPT)
Assign different roles to GPTs to form a collaborative software entity for complex tasks.

[How to Train an AI Chatbot With Custom Knowledge Base Using ChatGPT API](https://beebom.com/how-train-ai-chatbot-custom-knowledge-base-chatgpt-api/)

[Azure ChatGPT](https://github.com/NEYRIB/azurechatgpt)
ChatGPT has grown explosively in popularity as we all know now. Business users across the globe often tap into the public service to work more productively or act as a creative assistant.

However, ChatGPT risks exposing confidential intellectual property. One option is to block corporate access to ChatGPT, but people always find workarounds. This also limits the powerful capabilities of ChatGPT and reduces employee productivity and their work experience.

Azure ChatGPT is our enterprise option. This is the exact same service but offered as your private ChatGPT.

[OpenLLaMA: An Open Reproduction of LLaMA](https://github.com/openlm-research/open_llama)
we are releasing our public preview of OpenLLaMA, a permissively licensed open source reproduction of Meta AI’s LLaMA. We are releasing a series of 3B, 7B and 13B models trained on different data mixtures. Our model weights can serve as the drop in replacement of LLaMA in existing implementations.

In this repo, we present a permissively licensed open source reproduction of Meta AI's [LLaMA](https://ai.facebook.com/blog/large-language-model-llama-meta-ai/) large language model. We are releasing a series of 3B, 7B and 13B models trained on 1T tokens. We provide PyTorch and JAX weights of pre-trained OpenLLaMA models, as well as evaluation results and comparison against the original LLaMA models. The v2 model is better than the old v1 model trained on a different data mixture.

[LLaMA2-Accessory: An Open-source Toolkit for LLM Development](https://github.com/Alpha-VLLM/LLaMA2-Accessory)
**LLaMA2-Accessory** is an open-source toolkit for pre-training, fine-tuning and deployment of **Large Language Models (LLMs)** and **mutlimodal LLMs**. This repo is mainly inherited from [LLaMA-Adapter](https://github.com/OpenGVLab/LLaMA-Adapter) with more advanced features

[Code Llama](https://github.com/facebookresearch/codellama)
Code Llama is a family of large language models for code based on [Llama 2](https://github.com/facebookresearch/llama) providing state-of-the-art performance among open models, infilling capabilities, support for large input contexts, and zero-shot instruction following ability for programming tasks. We provide multiple flavors to cover a wide range of applications: foundation models (Code Llama), Python specializations (Code Llama - Python), and instruction-following models (Code Llama - Instruct) with 7B, 13B and 34B parameters each. All models are trained on sequences of 16k tokens and show improvements on inputs with up to 100k tokens. 7B and 13B Code Llama and Code Llama - Instruct variants support infilling based on surrounding content. Code Llama was developed by fine-tuning Llama 2 using a higher sampling of code. As with Llama 2, we applied considerable safety mitigations to the fine-tuned versions of the model. For detailed information on model training, architecture and parameters, evaluations, responsible AI and safety refer to our [research paper](https://ai.meta.com/research/publications/code-llama-open-foundation-models-for-code/). Output generated by code generation features of the Llama Materials, including Code Llama, may be subject to third party licenses, including, without limitation, open source licenses.